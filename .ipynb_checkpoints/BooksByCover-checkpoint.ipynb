{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUDGING A BOOK BY ITS COVER..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for comparing histograms\n",
    "correl_threshold = 0.9\n",
    "\n",
    "# Parameters for SSIM comparison\n",
    "similarity_index_threshold = 0.0\n",
    "ssim_matches_limit = 1000\n",
    "\n",
    "# Parameters for SIFT comparision\n",
    "sift_features_limit = 1000\n",
    "lowe_ratio = 0.75\n",
    "predictions_count = 4\n",
    "\n",
    "# Parameters to display results\n",
    "query_image_number = 0\n",
    "amazon_reviews_count = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading train and query images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imlist(path):\n",
    "    \"\"\"\n",
    "    The function imlist returns all the names of the files in \n",
    "    the directory path supplied as argument to the function.\n",
    "    \"\"\"\n",
    "    return [os.path.join(path, f) for f in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(im_title, im):\n",
    "    ''' This is function to display the image'''\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(im_title)\n",
    "    plt.axis(\"off\")\n",
    "    if len(im.shape) == 2:\n",
    "        plt.imshow(im, cmap = \"gray\")\n",
    "    else:\n",
    "        im_display = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n",
    "        plt.imshow(im_display)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths = []\n",
    "train_path = \"C:/Users/ckkok/Desktop/LibrarySearch/downloads/oreilly\"\n",
    "for root, dirs, files in os.walk(train_path):\n",
    "     for file in files:\n",
    "        train_paths.append((os.path.join(root, file)))\n",
    "len(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_path = \"C:/Users/ckkok/Desktop/LibrarySearch/book_db\"\n",
    "query_paths = imlist(query_path)\n",
    "len(query_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB 3D 8-bin Color histogram of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_train = []\n",
    "\n",
    "for path in train_paths:\n",
    "    image = cv2.imread(path)\n",
    "   \n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # extract a 3D RGB color histogram from the image,\n",
    "    # using 8 bins per channel, normalize, and update\n",
    "    # the index\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8],[0, 256, 0, 256, 0, 256])\n",
    "    hist = cv2.normalize(hist, None)\n",
    "    hist_train.append((path,hist))\n",
    "    \n",
    "len(hist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the train data histograms to a pickle file\n",
    "\n",
    "with open('train_hist_data.pkl', 'wb') as f:\n",
    "    pickle.dump(hist_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train data histograms from pickle file\n",
    "\n",
    "with open('train_hist_data.pkl', 'rb') as f:\n",
    "    hist_train = pickle.load(f)\n",
    "\n",
    "len(hist_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB 3D 8-bin Color histogram of query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_query = []\n",
    "for path in query_paths:\n",
    "    image = cv2.imread(path)\n",
    "    \n",
    "    if image is None:\n",
    "        continue\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # extract a 3D RGB color histogram from the image,\n",
    "    # using 8 bins per channel, normalize, and update the index\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8],[0, 256, 0, 256, 0, 256])\n",
    "    hist = cv2.normalize(hist, None)\n",
    "    hist_query.append((path,hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating matches for query images from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_matches = []\n",
    "for i in range(len(hist_query)):\n",
    "    matches = []\n",
    "    for j in range(len(hist_train)):\n",
    "        cmp = cv2.compareHist(hist_query[i][1], hist_train[j][1], cv2.HISTCMP_CORREL)\n",
    "        if cmp > correl_threshold:\n",
    "            matches.append((cmp,hist_train[j][0]))\n",
    "    matches.sort(key=lambda x : x[0] , reverse = True)\n",
    "    hist_matches.append((hist_query[i][0],matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating efficiency of color histogram matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_count_hist = 0\n",
    "miss_count_hist = 0\n",
    "\n",
    "for i in range(len(hist_matches)):\n",
    "    q_text = hist_matches[i][0].split(\"/\")[-1]\n",
    "    p_text = []\n",
    "    for j in range(len(hist_matches[i][1])):\n",
    "        text = hist_matches[i][1][j][1].split(\"/\")[-1]\n",
    "        p_text.append(text)\n",
    "    if q_text in p_text:\n",
    "        hit_count_hist += 1\n",
    "    else:\n",
    "        miss_count_hist += 1\n",
    "\n",
    "print(\"query set is \", query_path.split(\"/\")[-1])        \n",
    "print(\"\\n\")\n",
    "print(\"Total number of images = \",len(hist_matches))\n",
    "print(\"Number of correctly retrieved images = \",hit_count_hist)\n",
    "print(\"Number of images that could not be retrieved = \",miss_count_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Similarity Index Matching (SSIM) on query images and their matches from color histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_index(q_path,m_path):\n",
    "    q_i = cv2.imread(q_path,0)\n",
    "    q_i = cv2.resize(q_i,(8,8))\n",
    "    m_i = cv2.imread(m_path,0)\n",
    "    m_i = cv2.resize(m_i,(8,8))\n",
    "    return ssim(q_i,m_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ssim_matches = []\n",
    "\n",
    "for i in range(len(hist_matches)):\n",
    "    query_image_path = hist_matches[i][0]\n",
    "    matches = []\n",
    "    for j in range(len(hist_matches[i][1])):\n",
    "        match_image_path = hist_matches[i][1][j][1]\n",
    "        si = similarity_index(query_image_path,match_image_path)\n",
    "        if si > similarity_index_threshold:\n",
    "            matches.append((si,match_image_path))\n",
    "    matches.sort(key=lambda x : x[0] , reverse = True)\n",
    "    ssim_matches.append((query_image_path,matches[:ssim_matches_limit]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating efficiency of SSIM matching with ssim_matches_limit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_count_ssim = 0\n",
    "miss_count_ssim = 0\n",
    "\n",
    "for i in range(len(ssim_matches)):\n",
    "    q_text = ssim_matches[i][0].split(\"/\")[-1]\n",
    "    p_text = []\n",
    "    for j in range(len(ssim_matches[i][1])):\n",
    "        text = ssim_matches[i][1][j][1].split(\"/\")[-1]\n",
    "        p_text.append(text)\n",
    "    if q_text in p_text:\n",
    "        hit_count_ssim += 1\n",
    "    else:\n",
    "        miss_count_ssim += 1\n",
    "\n",
    "print(\"query set is \", query_path.split(\"/\")[-1])        \n",
    "print(\"\\n\")\n",
    "print(\"Total number of images = \",len(ssim_matches))\n",
    "print(\"Number of correctly retrieved images = \",hit_count_ssim)\n",
    "print(\"Number of images that could not be retrieved = \",miss_count_ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANN comparision of SIFT features of query images and their matches from SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sift_features(image):\n",
    "    sift = cv2.xfeatures2d.SIFT_create(sift_features_limit)\n",
    "    # kp is the keypoints\n",
    "    #\n",
    "    # desc is the SIFT descriptors, they're 128-dimensional vectors\n",
    "    # that we can use for our final features\n",
    "    kp, desc = sift.detectAndCompute(image, None)\n",
    "    return kp, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN matcher\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(ssim_matches)):\n",
    "    matches_flann = []\n",
    "    # Reading query image\n",
    "    q_path = ssim_matches[i][0]\n",
    "    q_img = cv2.imread(q_path)\n",
    "    if q_img is None:\n",
    "        continue\n",
    "    q_img = cv2.cvtColor(q_img, cv2.COLOR_BGR2RGB)\n",
    "    # Generating SIFT features for query image\n",
    "    q_kp,q_des = gen_sift_features(q_img)\n",
    "    if q_des is None:\n",
    "        continue\n",
    "    \n",
    "    for j in range(len(ssim_matches[i][1])):\n",
    "        matches_count = 0\n",
    "        m_path = ssim_matches[i][1][j][1]\n",
    "        m_img = cv2.imread(m_path)        \n",
    "        if m_img is None:\n",
    "            continue\n",
    "        m_img = cv2.cvtColor(m_img, cv2.COLOR_BGR2RGB)\n",
    "        # Generating SIFT features for predicted ssim images\n",
    "        m_kp,m_des = gen_sift_features(m_img)\n",
    "        if m_des is None:\n",
    "            continue\n",
    "        # Calculating number of feature matches using FLANN\n",
    "        matches = flann.knnMatch(q_des,m_des,k=2)\n",
    "        #ratio query as per Lowe's paper\n",
    "        matches_count = 0\n",
    "        for x,(m,n) in enumerate(matches):\n",
    "            if m.distance < lowe_ratio*n.distance:\n",
    "                matches_count += 1\n",
    "        matches_flann.append((matches_count,m_path))\n",
    "    matches_flann.sort(key=lambda x : x[0] , reverse = True)\n",
    "    predictions.append((q_path,matches_flann[:predictions_count]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating final efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_count = 0\n",
    "miss_count = 0\n",
    "\n",
    "hit_top_1 = 0\n",
    "hit_top_2 = 0\n",
    "hit_top_3 = 0\n",
    "hit_top_4 = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    q_text = predictions[i][0].split(\"/\")[-1]\n",
    "    p_text = []\n",
    "    if len(predictions[i][1]) == 0:\n",
    "        miss_count += 1\n",
    "        continue\n",
    "    for j in range(len(predictions[i][1])):\n",
    "        text = predictions[i][1][j][1].split(\"/\")[-1]\n",
    "        p_text.append(text)\n",
    "    try:\n",
    "        if q_text == p_text[0]:\n",
    "            hit_top_1 += 1\n",
    "        elif q_text == p_text[1]:\n",
    "            hit_top_2 += 1 \n",
    "        elif q_text == p_text[2]:\n",
    "            hit_top_3 += 1 \n",
    "        elif q_text == p_text[3]:\n",
    "            hit_top_4 += 1 \n",
    "        else:\n",
    "            miss_count += 1\n",
    "    except IndexError:\n",
    "        miss_count += 1\n",
    "\n",
    "predic_len = 100\n",
    "\n",
    "hit_top_2 += hit_top_1\n",
    "hit_top_3 += hit_top_2\n",
    "hit_top_4 += hit_top_3\n",
    "hit_count = hit_top_4\n",
    "\n",
    "\n",
    "print(\"query set is \", query_path.split(\"/\")[-1])        \n",
    "print(\"\\n\")\n",
    "print(\"Number of correctly retrieved images = \",hit_count)\n",
    "print(\"Number of images that could not be retrieved = \",miss_count)\n",
    "print(\"\\n\")\n",
    "print(f\"Accuracy @ Top 1 predictions = {hit_top_1*100/predic_len}%\")\n",
    "print(f\"Accuracy @ Top 2 predictions = {hit_top_2*100/predic_len}%\")\n",
    "print(f\"Accuracy @ Top 3 predictions = {hit_top_3*100/predic_len}%\")\n",
    "print(f\"Accuracy @ Top 4 predictions = {hit_top_4*100/predic_len}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = [hit_top_1,hit_top_2,hit_top_3,hit_top_4]\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "ax = freq_series.plot(kind='bar', figsize=(10,7),\n",
    "                                        color=\"brown\", fontsize=13);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title('Top-k Predictions',fontsize=18)\n",
    "ax.set_ylabel('Accuracy',fontsize=18)\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "# set individual bar lables using above list\n",
    "for i in ax.patches:\n",
    "    # get_x pulls left or right; get_height pushes up or down\n",
    "    ax.text(i.get_x()+0.15, i.get_height()+.5, \\\n",
    "            str(round((i.get_height()), 2))+'%', fontsize=15,\n",
    "                color='dimgrey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying predicted images for query image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = \"\"\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# Loading query image\n",
    "query_image = cv2.imread(predictions[query_image_number][0])\n",
    "query_image = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.axis('off')\n",
    "plt.title(\"Query Image\",fontsize=15)\n",
    "plt.imshow(query_image)\n",
    "\n",
    "top_prediction_image = cv2.imread(predictions[query_image_number][1][0][1])\n",
    "top_prediction_image = cv2.cvtColor(top_prediction_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Loading predicted images\n",
    "for i in range(predictions_count):\n",
    "    title_text = \"\"\n",
    "    img = cv2.imread(predictions[query_image_number][1][i][1])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    title_text = \"Matches = \" + str(predictions[query_image_number][1][i][0])\n",
    "    plt.subplot(2,4,5+i)\n",
    "    plt.axis('off')\n",
    "    plt.text(0.5,-0.1,title_text,ha='center',va='center',fontsize=15,transform=plt.subplot(2,4,5+i).transAxes)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing keypoints and matches for top predicted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "q_kp,q_des = gen_sift_features(query_image)\n",
    "m_kp,m_des = gen_sift_features(top_prediction_image)\n",
    "matches_old = flann.knnMatch(q_des,m_des,k=2)\n",
    "matches = random.sample(matches_old, 500)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < lowe_ratio*n.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "        \n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags = 0)\n",
    "\n",
    "kp_img = cv2.drawMatchesKnn(query_image,q_kp,top_prediction_image,m_kp,matches,None,**draw_params)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(kp_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top predicted book details from Goodreads & Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn = predictions[query_image_number][1][0][1].split(\"/\")[-1].split(\".\")[0]\n",
    "print(isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_details_goodreads(isbn):\n",
    "    \n",
    "    # Getting book title from Amazon\n",
    "    amazon_base_url = \"https://www.amazon.com/dp/\"\n",
    "    amazon_url = amazon_base_url + isbn\n",
    "    req = Request(amazon_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urlopen(req).read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    a_title = soup.find_all(\"span\",id=\"productTitle\")\n",
    "    a_title = str(a_title)\n",
    "    a_title = remove_tags(a_title)\n",
    "    a_title = a_title.strip(\"[]\")\n",
    "    a_title = a_title.strip()\n",
    "    a_title_list = a_title.split(\" \")\n",
    "    \n",
    "    # Goodreads Scraping\n",
    "    goodreads_base_url = \"https://www.goodreads.com/book/isbn/\"\n",
    "    goodreads_url = goodreads_base_url + isbn\n",
    "    req = Request(goodreads_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urlopen(req).read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Book Title\n",
    "    book_name = soup.find(itemprop=\"name\")\n",
    "    book_name = str(book_name)\n",
    "    book_name = remove_tags(book_name)\n",
    "    book_name = book_name.strip()\n",
    "    book_name_list = book_name.split(\" \")\n",
    "    \n",
    "    # Verifying if isbn is present in Goodreads by comparing book titles from Goodreads & Amazon\n",
    "#     if str(a_title_list[0]).lower() != str(book_name_list[0]).lower():\n",
    "#         return print(\"Book not found in Goodreads\")\n",
    "    \n",
    "    # Author Names\n",
    "    author_names = soup.find_all(\"span\",itemprop=\"name\")\n",
    "    author_names = str(author_names)\n",
    "    author_names = author_names.split(\",\")\n",
    "    author_name = author_names[0]\n",
    "    author_name = author_name.split(\">\")[1].split(\"<\")[0]\n",
    "    for i in range(len(author_names)):\n",
    "        author_names[i] = author_names[i].split(\">\")[1].split(\"<\")[0]\n",
    "    \n",
    "    author_names_text = \"\"\n",
    "    for i in range(len(author_names)):\n",
    "        author_names_text += str(author_names[i])\n",
    "        author_names_text += \", \"\n",
    "        \n",
    "    # Number of Ratings\n",
    "    rating_count = soup.find(itemprop=\"ratingCount\")\n",
    "    rating_count = str(rating_count)\n",
    "    rating_count = rating_count.split('\"')[1]\n",
    "\n",
    "    # Average Rating\n",
    "    rating_val = soup.find(itemprop=\"ratingValue\")\n",
    "    rating_val = str(rating_val)\n",
    "    rating_val = remove_tags(rating_val)\n",
    "    \n",
    "    # Number of pages in book\n",
    "    pg_count = soup.find(\"meta\",  property=\"books:page_count\")\n",
    "    pg_count = str(pg_count)\n",
    "    pg_count = pg_count.split('\"')[1]\n",
    "    \n",
    "    # Book Description\n",
    "    desc = soup.find(\"div\", id=\"description\")\n",
    "    if desc is not None:\n",
    "        desc = desc.find_all(\"span\",style=\"display:none\")\n",
    "        if desc is not None:\n",
    "            desc = str(desc)\n",
    "            desc = remove_tags(desc)\n",
    "            description = desc.strip(\"[]\")\n",
    "            description = description.strip()\n",
    "        else:\n",
    "            description = \"No description found\"\n",
    "    else:\n",
    "        description = \"No description found\"\n",
    "\n",
    "    # Printing book details from Goodreads\n",
    "    printmd('**Book Details from Goodreads\\n**')\n",
    "    #print(\"Book Details from Goodreads\\n\")\n",
    "    print(\"Book Title: \",book_name.splitlines()[0])\n",
    "    #print(\"\\n\")\n",
    "    print(\"Authors: \",author_names_text)\n",
    "    #print(\"\\n\")\n",
    "    print(\"Average Rating: \",rating_val)\n",
    "    #print(\"\\n\")\n",
    "    print(\"Number of ratings: \",rating_count)\n",
    "    #print(\"\\n\")\n",
    "    print(\"Number of pages in book: \",pg_count)\n",
    "    print(\"\\n\")\n",
    "    print(\"Book Description:\")\n",
    "    print(\"\\n\")\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_details_amazon(isbn):\n",
    "    \n",
    "    # Amazon Scraping\n",
    "    amazon_base_url = \"https://www.amazon.com/dp/\"\n",
    "    amazon_url = amazon_base_url + isbn\n",
    "    req = Request(amazon_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urlopen(req).read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Book title\n",
    "    a_title = soup.find_all(\"span\",id=\"productTitle\")\n",
    "    a_title = str(a_title)\n",
    "    a_title = remove_tags(a_title)\n",
    "    a_title = a_title.strip(\"[]\")\n",
    "    a_title = a_title.strip()\n",
    "    \n",
    "    # Book details\n",
    "    book_info = []\n",
    "    for li in soup.select('table#productDetailsTable div.content ul li'):\n",
    "        try:\n",
    "            title = li.b\n",
    "            key = title.text.strip().rstrip(':')\n",
    "            value = title.next_sibling.strip()\n",
    "            value = value.strip(\"()\")\n",
    "            book_info.append((key,value))\n",
    "        except AttributeError:\n",
    "            break\n",
    "            \n",
    "    # Amazon reviews scraping\n",
    "    amazon_review_base_url = \"https://www.amazon.com/product-reviews/\"\n",
    "    amazon_review_url = amazon_review_base_url + isbn + \"/ref=cm_cr_getr_d_paging_btm_2?pageNumber=\"\n",
    "    req = Request(amazon_review_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urlopen(req).read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # List of book reviews in Amazon\n",
    "    reviews_list = []\n",
    "    reviews_list_final = []\n",
    "    for pg in range(1,5):\n",
    "        amazon_review_url = amazon_review_base_url + isbn + \"/ref=cm_cr_getr_d_paging_btm_2?pageNumber=\" + str(pg)\n",
    "        req = Request(amazon_review_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        page = urlopen(req).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        txt = soup.find(\"div\", id=\"cm_cr-review_list\")\n",
    "        try:\n",
    "            for rawreview in txt.find_all('span', {'class' : 'a-size-base review-text'}):\n",
    "                text = rawreview.parent.parent.parent.text\n",
    "                startindex = text.index('5 stars') + 7\n",
    "                endindex = text.index('Was this review helpful to you?')\n",
    "                text = text[startindex:endindex]\n",
    "                text = text.split(\"Verified Purchase\")[1]\n",
    "                rText = text.split(\".\")[:-1]\n",
    "                review_text = \"\"\n",
    "                for i in range(len(rText)):\n",
    "                    review_text += rText[i]\n",
    "                    review_text += \".\"\n",
    "                if review_text is not \"\":\n",
    "                    if \"|\" not in review_text:\n",
    "                        reviews_list.append(review_text)\n",
    "                    else:\n",
    "                        rText = text.split(\".\")[:-2]\n",
    "                        review_text = \"\"\n",
    "                        for x in range(len(rText)):\n",
    "                            review_text += rText[x]\n",
    "                            review_text += \".\"\n",
    "                        reviews_list.append(review_text)\n",
    "        except AttributeError:\n",
    "            review_text = \"No reviews found.\"\n",
    "    \n",
    "    if amazon_reviews_count < len(reviews_list):\n",
    "        reviews_list_final = reviews_list[:amazon_reviews_count]\n",
    "    else:\n",
    "        reviews_list_final = reviews_list\n",
    "        \n",
    "    # Printing book details from Amazon\n",
    "    printmd('**Book Details from Amazon\\n**')\n",
    "    #print(\"Book Details from Amazon\\n\")\n",
    "    print(\"Book Title: \",a_title)\n",
    "    #print(\"\\n\")\n",
    "    for i in range(len(book_info)):\n",
    "        print(f\"{book_info[i][0]} : {book_info[i][1]}\")\n",
    "        #print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    if len(reviews_list_final) == 0:\n",
    "        print(review_text)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"Displaying top {amazon_reviews_count} book reviews:\\n\")\n",
    "        for i in range(len(reviews_list_final)):\n",
    "            review_txt_list = reviews_list_final[i].split(\".\")[:3]\n",
    "            review_txt = \"\"\n",
    "            for j in range(len(review_txt_list)):\n",
    "                review_txt += review_txt_list[j]\n",
    "                review_txt += \".\"\n",
    "            review_txt += \"..\"\n",
    "            print(review_txt)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_details_goodreads(isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_details_amazon(isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
